{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c380543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, RGCNConv\n",
    "from torch_geometric.utils import negative_sampling, to_undirected\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d7ba546",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalGCNRecommender(nn.Module):\n",
    "    \"\"\"\n",
    "    RE-GCN style temporal recommender for product recommendations\n",
    "    \n",
    "    This model:\n",
    "    1. Uses recurrent GCN layers to capture temporal dynamics\n",
    "    2. Predicts user-product interactions and their weights (quantities)\n",
    "    3. Handles bipartite user-product graphs with temporal evolution\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_users, num_products, node_features_dim, hidden_dim=64, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_users = num_users\n",
    "        self.num_products = num_products\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Node embeddings\n",
    "        self.user_embedding = nn.Embedding(num_users, hidden_dim)\n",
    "        self.product_embedding = nn.Embedding(num_products, hidden_dim)\n",
    "        \n",
    "        # Feature projection\n",
    "        self.feature_proj = nn.Linear(node_features_dim, hidden_dim)\n",
    "        \n",
    "        # Temporal GCN layers (recurrent style)\n",
    "        self.gcn_layers = nn.ModuleList([\n",
    "            GCNConv(hidden_dim, hidden_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Recurrent cells for temporal modeling\n",
    "        self.gru_cells = nn.ModuleList([\n",
    "            nn.GRUCell(hidden_dim, hidden_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Edge weight predictor (for quantity prediction)\n",
    "        self.edge_weight_predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            #nn.Sigmoid()  # Normalize weights\n",
    "        )\n",
    "        \n",
    "        # Link prediction (binary classification)\n",
    "        self.link_predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize hidden states\n",
    "        self.reset_hidden_states()\n",
    "    \n",
    "    def reset_hidden_states(self):\n",
    "        \"\"\"Reset hidden states for new sequence\"\"\"\n",
    "        self.hidden_states = [None] * self.num_layers\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr, user_indices, product_indices):\n",
    "        \"\"\"\n",
    "        Forward pass for temporal link prediction\n",
    "        \n",
    "        Args:\n",
    "            x: Node features [num_nodes, feature_dim]\n",
    "            edge_index: Edge indices [2, num_edges]\n",
    "            edge_attr: Edge attributes [num_edges, 2] (quantity, timestamp)\n",
    "            user_indices: User node indices for prediction\n",
    "            product_indices: Product node indices for prediction\n",
    "        \"\"\"\n",
    "        batch_size = len(user_indices)\n",
    "        \n",
    "        # Project features to hidden dimension\n",
    "        h = self.feature_proj(x)\n",
    "        \n",
    "        # Add positional embeddings\n",
    "        user_emb = self.user_embedding(torch.arange(self.num_users, device=x.device))\n",
    "        product_emb = self.product_embedding(torch.arange(self.num_products, device=x.device))\n",
    "        \n",
    "        h[:self.num_users] += user_emb\n",
    "        h[self.num_users:] += product_emb\n",
    "        \n",
    "        # Temporal GCN with recurrent connections\n",
    "        for layer_idx, (gcn, gru) in enumerate(zip(self.gcn_layers, self.gru_cells)):\n",
    "            # Graph convolution\n",
    "            h_new = gcn(h, edge_index)\n",
    "            h_new = F.relu(h_new)\n",
    "            h_new = self.dropout(h_new)\n",
    "            \n",
    "            # Recurrent update (temporal modeling)\n",
    "            if self.hidden_states[layer_idx] is not None:\n",
    "                h_new = gru(h_new, self.hidden_states[layer_idx])\n",
    "            else:\n",
    "                h_new = gru(h_new, torch.zeros_like(h_new))\n",
    "            \n",
    "            # Update hidden state\n",
    "            self.hidden_states[layer_idx] = h_new.detach()\n",
    "            h = h_new\n",
    "        \n",
    "        # Extract user and product embeddings for prediction\n",
    "        user_embeddings = h[user_indices]\n",
    "        product_embeddings = h[product_indices]\n",
    "        \n",
    "        # Concatenate user and product embeddings\n",
    "        edge_embeddings = torch.cat([user_embeddings, product_embeddings], dim=1)\n",
    "        \n",
    "        # Predict link existence and edge weight\n",
    "        link_scores = self.link_predictor(edge_embeddings).squeeze()\n",
    "        edge_weights = self.edge_weight_predictor(edge_embeddings).squeeze()\n",
    "        \n",
    "        return link_scores, edge_weights, h\n",
    "\n",
    "class TemporalRecommenderTrainer:\n",
    "    \"\"\"Training and evaluation pipeline for temporal recommendations\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tkg_data, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.tkg_data = tkg_data\n",
    "        \n",
    "        # Prepare temporal snapshots\n",
    "        self.temporal_snapshots = self.create_temporal_snapshots()\n",
    "        \n",
    "    def create_temporal_snapshots(self, time_window_hours=24):\n",
    "        \"\"\"\n",
    "        Create temporal snapshots from TKG data\n",
    "        \n",
    "        Args:\n",
    "            time_window_hours: Hours per snapshot\n",
    "        \"\"\"\n",
    "        quadruples = self.tkg_data['quadruples']\n",
    "        \n",
    "        # Group quadruples by time windows\n",
    "        snapshots = {}\n",
    "        min_time = min(q[3] for q in quadruples)\n",
    "        \n",
    "        for user_id, quantity, product_id, timestamp in quadruples:\n",
    "            # Calculate time window\n",
    "            hours_since_start = (timestamp - min_time).total_seconds() / 3600\n",
    "            window_id = int(hours_since_start // time_window_hours)\n",
    "            \n",
    "            if window_id not in snapshots:\n",
    "                snapshots[window_id] = []\n",
    "            \n",
    "            snapshots[window_id].append((user_id, quantity, product_id, timestamp))\n",
    "        \n",
    "        # Convert to sorted list\n",
    "        sorted_snapshots = [snapshots[i] for i in sorted(snapshots.keys())]\n",
    "        \n",
    "        print(f\"Created {len(sorted_snapshots)} temporal snapshots\")\n",
    "        return sorted_snapshots\n",
    "    \n",
    "    def prepare_snapshot_data(self, snapshot_edges):\n",
    "        \"\"\"Prepare PyG data for a single snapshot\"\"\"\n",
    "        \n",
    "        # Extract edges and attributes\n",
    "        edge_list = []\n",
    "        edge_weights = []\n",
    "        \n",
    "        for user_id, quantity, product_id, timestamp in snapshot_edges:\n",
    "            edge_list.append([user_id, product_id])\n",
    "            edge_weights.append(quantity)\n",
    "        \n",
    "        if len(edge_list) == 0:\n",
    "            return None, None, None\n",
    "        \n",
    "        edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_weights, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Make undirected for better message passing\n",
    "        # edge_index = to_undirected(edge_index)\n",
    "        \n",
    "        return edge_index, edge_attr, edge_list\n",
    "    \n",
    "    def create_negative_samples(self, positive_edges, num_negative=None):\n",
    "        \"\"\"Create negative samples for link prediction\"\"\"\n",
    "        \n",
    "        if num_negative is None:\n",
    "            num_negative = len(positive_edges)\n",
    "        \n",
    "        negative_edges = []\n",
    "        user_ids = set(edge[0] for edge in positive_edges)\n",
    "        product_ids = set(edge[1] for edge in positive_edges)\n",
    "        positive_set = set((edge[0], edge[1]) for edge in positive_edges)\n",
    "        \n",
    "        while len(negative_edges) < num_negative:\n",
    "            user_id = random.choice(list(user_ids))\n",
    "            product_id = random.choice(list(product_ids))\n",
    "            \n",
    "            if (user_id, product_id) not in positive_set:\n",
    "                negative_edges.append([user_id, product_id])\n",
    "                positive_set.add((user_id, product_id))  # Avoid duplicates\n",
    "        \n",
    "        return negative_edges\n",
    "    \n",
    "    def train_epoch(self, optimizer, train_snapshots):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Reset hidden states for new epoch\n",
    "        self.model.reset_hidden_states()\n",
    "        \n",
    "        for snapshot_idx, snapshot_edges in enumerate(tqdm(train_snapshots, desc=\"Training\")):\n",
    "            \n",
    "            edge_index, edge_attr, edge_list = self.prepare_snapshot_data(snapshot_edges)\n",
    "            \n",
    "            if edge_index is None:\n",
    "                continue\n",
    "            \n",
    "            # Move to device\n",
    "            x = self.tkg_data['x'].to(self.device)\n",
    "            edge_index = edge_index.to(self.device)\n",
    "            edge_attr = edge_attr.to(self.device)\n",
    "            \n",
    "            # Create positive and negative samples\n",
    "            positive_edges = edge_list\n",
    "            negative_edges = self.create_negative_samples(positive_edges)\n",
    "            \n",
    "            # Prepare training data\n",
    "            all_edges = positive_edges + negative_edges\n",
    "            labels = torch.cat([\n",
    "                torch.ones(len(positive_edges)),\n",
    "                torch.zeros(len(negative_edges))\n",
    "            ]).to(self.device)\n",
    "            \n",
    "            # Extract edge weights (only for positive edges) - NO NORMALIZATION\n",
    "            true_weights = torch.tensor([edge[1] for edge in snapshot_edges], dtype=torch.float).to(self.device)\n",
    "            \n",
    "            # Get user and product indices\n",
    "            user_indices = torch.tensor([edge[0] for edge in all_edges], dtype=torch.long).to(self.device)\n",
    "            product_indices = torch.tensor([edge[1] for edge in all_edges], dtype=torch.long).to(self.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            link_scores, pred_weights, _ = self.model(x, edge_index, edge_attr, user_indices, product_indices)\n",
    "            \n",
    "            # Link prediction loss\n",
    "            link_loss = F.binary_cross_entropy(link_scores, labels)\n",
    "            \n",
    "            # Edge weight loss (only for positive edges)\n",
    "            if len(true_weights) > 0:\n",
    "                weight_loss = F.mse_loss(pred_weights[:len(positive_edges)], true_weights)\n",
    "            else:\n",
    "                weight_loss = torch.tensor(0.0, device=self.device)\n",
    "            \n",
    "            # Combined loss\n",
    "            total_loss_batch = link_loss + 0.1 * weight_loss  # Reduced weight loss coefficient\n",
    "            \n",
    "            total_loss_batch.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += total_loss_batch.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        return total_loss / max(num_batches, 1)\n",
    "\n",
    "    def evaluate(self, test_snapshots):\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        all_link_preds = []\n",
    "        all_link_labels = []\n",
    "        all_weight_preds = []\n",
    "        all_weight_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Reset hidden states\n",
    "            self.model.reset_hidden_states()\n",
    "            \n",
    "            for snapshot_edges in tqdm(test_snapshots, desc=\"Evaluating\"):\n",
    "                \n",
    "                edge_index, edge_attr, edge_list = self.prepare_snapshot_data(snapshot_edges)\n",
    "                \n",
    "                if edge_index is None:\n",
    "                    continue\n",
    "                \n",
    "                # Move to device\n",
    "                x = self.tkg_data['x'].to(self.device)\n",
    "                edge_index = edge_index.to(self.device)\n",
    "                edge_attr = edge_attr.to(self.device)\n",
    "                \n",
    "                # Create test samples\n",
    "                positive_edges = edge_list\n",
    "                negative_edges = self.create_negative_samples(positive_edges)\n",
    "                \n",
    "                all_edges = positive_edges + negative_edges\n",
    "                labels = torch.cat([\n",
    "                    torch.ones(len(positive_edges)),\n",
    "                    torch.zeros(len(negative_edges))\n",
    "                ])\n",
    "                \n",
    "                # True weights - NO NORMALIZATION\n",
    "                true_weights = torch.tensor([q for _, q, _, _ in snapshot_edges], dtype=torch.float)\n",
    "                \n",
    "                # Get indices\n",
    "                user_indices = torch.tensor([edge[0] for edge in all_edges], dtype=torch.long).to(self.device)\n",
    "                product_indices = torch.tensor([edge[1] for edge in all_edges], dtype=torch.long).to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                link_scores, pred_weights, _ = self.model(x, edge_index, edge_attr, user_indices, product_indices)\n",
    "                \n",
    "                # Collect predictions\n",
    "                all_link_preds.extend(link_scores.cpu().numpy())\n",
    "                all_link_labels.extend(labels.numpy())\n",
    "                \n",
    "                if len(true_weights) > 0:\n",
    "                    all_weight_preds.extend(pred_weights[:len(positive_edges)].cpu().numpy())\n",
    "                    all_weight_labels.extend(true_weights.numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if len(all_link_preds) > 0:\n",
    "            link_auc = roc_auc_score(all_link_labels, all_link_preds)\n",
    "            link_ap = average_precision_score(all_link_labels, all_link_preds)\n",
    "        else:\n",
    "            link_auc = link_ap = 0.0\n",
    "        \n",
    "        if len(all_weight_preds) > 0:\n",
    "            weight_mse = np.mean((np.array(all_weight_preds) - np.array(all_weight_labels)) ** 2)\n",
    "            weight_mae = np.mean(np.abs(np.array(all_weight_preds) - np.array(all_weight_labels)))\n",
    "        else:\n",
    "            weight_mse = weight_mae = 0.0\n",
    "        \n",
    "        return {\n",
    "            'link_auc': link_auc,\n",
    "            'link_ap': link_ap,\n",
    "            'weight_mse': weight_mse,\n",
    "            'weight_mae': weight_mae\n",
    "        }\n",
    "    \n",
    "    def train(self, num_epochs=50, lr=0.01, train_ratio=0.8):\n",
    "        \"\"\"Full training pipeline\"\"\"\n",
    "        \n",
    "        # Split temporal snapshots\n",
    "        split_idx = int(len(self.temporal_snapshots) * train_ratio)\n",
    "        train_snapshots = self.temporal_snapshots[:split_idx]\n",
    "        test_snapshots = self.temporal_snapshots[split_idx:]\n",
    "        \n",
    "        print(f\"Training on {len(train_snapshots)} snapshots, testing on {len(test_snapshots)} snapshots\")\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "        \n",
    "        best_auc = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            # Train\n",
    "            train_loss = self.train_epoch(optimizer, train_snapshots)\n",
    "            \n",
    "            # Evaluate\n",
    "            if epoch % 5 == 0:\n",
    "                metrics = self.evaluate(test_snapshots)\n",
    "                \n",
    "                print(f\"Epoch {epoch:3d}: Loss={train_loss:.4f}, \"\n",
    "                      f\"AUC={metrics['link_auc']:.4f}, \"\n",
    "                      f\"AP={metrics['link_ap']:.4f}, \"\n",
    "                      f\"Weight MSE={metrics['weight_mse']:.4f}\")\n",
    "                \n",
    "                if metrics['link_auc'] > best_auc:\n",
    "                    best_auc = metrics['link_auc']\n",
    "                    torch.save(self.model.state_dict(), 'best_temporal_recommender.pt')\n",
    "            \n",
    "            scheduler.step()\n",
    "        \n",
    "        # Load best model\n",
    "        self.model.load_state_dict(torch.load('best_temporal_recommender.pt'))\n",
    "        \n",
    "        # Final evaluation\n",
    "        final_metrics = self.evaluate(test_snapshots)\n",
    "        print(f\"\\nFinal Results:\")\n",
    "        print(f\"Link AUC: {final_metrics['link_auc']:.4f}\")\n",
    "        print(f\"Link AP: {final_metrics['link_ap']:.4f}\")\n",
    "        print(f\"Weight MSE: {final_metrics['weight_mse']:.4f}\")\n",
    "        \n",
    "        return final_metrics\n",
    "    \n",
    "    def recommend_products(self, user_id, top_k=10, exclude_existing=True):\n",
    "        \"\"\"\n",
    "        Generate product recommendations for a user\n",
    "        \n",
    "        Args:\n",
    "            user_id: Target user ID\n",
    "            top_k: Number of recommendations\n",
    "            exclude_existing: Whether to exclude products user already bought\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x = self.tkg_data['x'].to(self.device)\n",
    "            \n",
    "            # Get all products\n",
    "            all_product_ids = list(range(self.tkg_data['num_users'], \n",
    "                                       self.tkg_data['num_users'] + self.tkg_data['num_products']))\n",
    "            \n",
    "            # Exclude existing products if requested\n",
    "            if exclude_existing:\n",
    "                existing_products = set()\n",
    "                for quad in self.tkg_data['quadruples']:\n",
    "                    if quad[0] == user_id:  # user_id matches\n",
    "                        existing_products.add(quad[2])  # product_id\n",
    "                \n",
    "                candidate_products = [pid for pid in all_product_ids if pid not in existing_products]\n",
    "            else:\n",
    "                candidate_products = all_product_ids\n",
    "            \n",
    "            if len(candidate_products) == 0:\n",
    "                return []\n",
    "            \n",
    "            # Create dummy edge index (we need this for the forward pass)\n",
    "            dummy_edges = [[user_id, pid] for pid in candidate_products[:min(100, len(candidate_products))]]\n",
    "            edge_index = torch.tensor(dummy_edges, dtype=torch.long).t().contiguous().to(self.device)\n",
    "            edge_attr = torch.zeros((len(dummy_edges), 1), dtype=torch.float).to(self.device)\n",
    "            \n",
    "            # Prepare indices for prediction\n",
    "            user_indices = torch.tensor([user_id] * len(dummy_edges), dtype=torch.long).to(self.device)\n",
    "            product_indices = torch.tensor([edge[1] for edge in dummy_edges], dtype=torch.long).to(self.device)\n",
    "            \n",
    "            # Get predictions\n",
    "            link_scores, pred_weights, _ = self.model(x, edge_index, edge_attr, user_indices, product_indices)\n",
    "            \n",
    "            # Combine scores (link probability * predicted quantity)\n",
    "            combined_scores = link_scores * pred_weights\n",
    "            \n",
    "            # Get top-k recommendations\n",
    "            top_indices = torch.argsort(combined_scores, descending=True)[:top_k]\n",
    "            \n",
    "            recommendations = []\n",
    "            for idx in top_indices:\n",
    "                product_id = dummy_edges[idx][1]\n",
    "                score = combined_scores[idx].item()\n",
    "                link_prob = link_scores[idx].item()\n",
    "                pred_quantity = pred_weights[idx].item()\n",
    "                \n",
    "                recommendations.append({\n",
    "                    'product_id': product_id,\n",
    "                    'product_name': self.tkg_data.get('id_to_product', {}).get(product_id, f'Product_{product_id}'),\n",
    "                    'score': score,\n",
    "                    'link_probability': link_prob,\n",
    "                    'predicted_quantity': pred_quantity\n",
    "                })\n",
    "            \n",
    "            return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46d33e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Initialize model\\nmodel = TemporalGCNRecommender(\\n    num_users=tkg_data[\\'num_users\\'],\\n    num_products=tkg_data[\\'num_products\\'],\\n    node_features_dim=tkg_data[\\'x\\'].shape[1],\\n    hidden_dim=64\\n)\\n\\n# Initialize trainer\\ntrainer = TemporalRecommenderTrainer(model, tkg_data, device=\\'cuda\\' if torch.cuda.is_available() else \\'cpu\\')\\n\\n# Train the model\\nresults = trainer.train(num_epochs=30, lr=0.01)\\n\\n# Get recommendations for user\\nuser_id = 0  # First user\\nrecommendations = trainer.recommend_products(user_id, top_k=10)\\n\\nprint(f\"Top recommendations for user {user_id}:\")\\nfor i, rec in enumerate(recommendations, 1):\\n    print(f\"{i}. {rec[\\'product_name\\']}\")\\n    print(f\"   Score: {rec[\\'score\\']:.3f}, Link Prob: {rec[\\'link_probability\\']:.3f}, Pred Qty: {rec[\\'predicted_quantity\\']:.2f}\")\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your TKG data\n",
    "tkg_data = torch.load('graph/my_retail_tkg_pyg.pt', weights_only=False)\n",
    "\"\"\"\n",
    "# Initialize model\n",
    "model = TemporalGCNRecommender(\n",
    "    num_users=tkg_data['num_users'],\n",
    "    num_products=tkg_data['num_products'],\n",
    "    node_features_dim=tkg_data['x'].shape[1],\n",
    "    hidden_dim=64\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = TemporalRecommenderTrainer(model, tkg_data, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Train the model\n",
    "results = trainer.train(num_epochs=30, lr=0.01)\n",
    "\n",
    "# Get recommendations for user\n",
    "user_id = 0  # First user\n",
    "recommendations = trainer.recommend_products(user_id, top_k=10)\n",
    "\n",
    "print(f\"Top recommendations for user {user_id}:\")\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec['product_name']}\")\n",
    "    print(f\"   Score: {rec['score']:.3f}, Link Prob: {rec['link_probability']:.3f}, Pred Qty: {rec['predicted_quantity']:.2f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fdd72f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 305 temporal snapshots\n",
      "Training on 244 snapshots, testing on 61 snapshots\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 244/244 [00:23<00:00, 10.43it/s]\n",
      "Evaluating: 100%|██████████| 61/61 [00:03<00:00, 17.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0: Loss=0.6997, AUC=0.7929, AP=0.7284, Weight MSE=0.8309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 244/244 [00:30<00:00,  7.95it/s]\n",
      "Training: 100%|██████████| 244/244 [00:30<00:00,  8.00it/s]\n",
      "Training: 100%|██████████| 244/244 [00:29<00:00,  8.14it/s]\n",
      "Training: 100%|██████████| 244/244 [00:41<00:00,  5.83it/s]\n",
      "Training: 100%|██████████| 244/244 [00:41<00:00,  5.89it/s]\n",
      "Evaluating: 100%|██████████| 61/61 [00:05<00:00, 12.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5: Loss=0.4038, AUC=0.8680, AP=0.8225, Weight MSE=0.6761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 244/244 [00:48<00:00,  5.04it/s]\n",
      "Training: 100%|██████████| 244/244 [00:39<00:00,  6.15it/s]\n",
      "Training: 100%|██████████| 244/244 [00:46<00:00,  5.22it/s]\n",
      "Training: 100%|██████████| 244/244 [00:41<00:00,  5.88it/s]\n",
      "Training: 100%|██████████| 244/244 [00:45<00:00,  5.38it/s]\n",
      "Evaluating: 100%|██████████| 61/61 [00:06<00:00,  9.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10: Loss=0.3791, AUC=0.8734, AP=0.8255, Weight MSE=0.6745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 244/244 [00:39<00:00,  6.11it/s]\n",
      "Training: 100%|██████████| 244/244 [00:30<00:00,  8.13it/s]\n",
      "Training: 100%|██████████| 244/244 [00:26<00:00,  9.16it/s]\n",
      "Training: 100%|██████████| 244/244 [00:28<00:00,  8.42it/s]\n",
      "Training: 100%|██████████| 244/244 [00:29<00:00,  8.26it/s]\n",
      "Evaluating: 100%|██████████| 61/61 [00:03<00:00, 20.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  15: Loss=0.3675, AUC=0.8732, AP=0.8209, Weight MSE=0.6798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 244/244 [00:30<00:00,  8.03it/s]\n",
      "Training: 100%|██████████| 244/244 [00:30<00:00,  7.92it/s]\n",
      "Training: 100%|██████████| 244/244 [00:34<00:00,  7.05it/s]\n",
      "Training: 100%|██████████| 244/244 [00:33<00:00,  7.29it/s]\n",
      "Training: 100%|██████████| 244/244 [00:32<00:00,  7.40it/s]\n",
      "Evaluating: 100%|██████████| 61/61 [00:03<00:00, 18.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  20: Loss=0.3425, AUC=0.8784, AP=0.8363, Weight MSE=0.6887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 244/244 [00:36<00:00,  6.66it/s]\n",
      "Training: 100%|██████████| 244/244 [00:34<00:00,  7.05it/s]\n",
      "Training: 100%|██████████| 244/244 [00:36<00:00,  6.64it/s]\n",
      "Training: 100%|██████████| 244/244 [00:34<00:00,  7.10it/s]\n",
      "Training: 100%|██████████| 244/244 [00:35<00:00,  6.89it/s]\n",
      "Evaluating: 100%|██████████| 61/61 [00:04<00:00, 14.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  25: Loss=0.3260, AUC=0.9133, AP=0.8785, Weight MSE=0.6840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 244/244 [00:41<00:00,  5.93it/s]\n",
      "Training: 100%|██████████| 244/244 [00:39<00:00,  6.24it/s]\n",
      "Training: 100%|██████████| 244/244 [00:36<00:00,  6.68it/s]\n",
      "Training: 100%|██████████| 244/244 [00:36<00:00,  6.76it/s]\n",
      "Evaluating: 100%|██████████| 61/61 [00:03<00:00, 18.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Results:\n",
      "Link AUC: 0.9137\n",
      "Link AP: 0.8795\n",
      "Weight MSE: 0.6840\n",
      "Top recommendations for user 0:\n",
      "1. Product_4025\n",
      "   Score: 1.825, Link Prob: 0.903, Pred Qty: 2.02\n",
      "2. Product_3942\n",
      "   Score: 1.810, Link Prob: 0.905, Pred Qty: 2.00\n",
      "3. Product_3966\n",
      "   Score: 1.809, Link Prob: 0.904, Pred Qty: 2.00\n",
      "4. Product_3971\n",
      "   Score: 1.808, Link Prob: 0.921, Pred Qty: 1.96\n",
      "5. Product_3961\n",
      "   Score: 1.807, Link Prob: 0.904, Pred Qty: 2.00\n",
      "6. Product_3970\n",
      "   Score: 1.806, Link Prob: 0.901, Pred Qty: 2.00\n",
      "7. Product_4037\n",
      "   Score: 1.805, Link Prob: 0.898, Pred Qty: 2.01\n",
      "8. Product_4016\n",
      "   Score: 1.804, Link Prob: 0.904, Pred Qty: 2.00\n",
      "9. Product_4024\n",
      "   Score: 1.803, Link Prob: 0.898, Pred Qty: 2.01\n",
      "10. Product_3931\n",
      "   Score: 1.800, Link Prob: 0.904, Pred Qty: 1.99\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = TemporalGCNRecommender(\n",
    "    num_users=tkg_data['num_users'],\n",
    "    num_products=tkg_data['num_products'],\n",
    "    node_features_dim=tkg_data['x'].shape[1],\n",
    "    hidden_dim=64\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = TemporalRecommenderTrainer(model, tkg_data, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Train the model\n",
    "results = trainer.train(num_epochs=30, lr=0.01)\n",
    "\n",
    "# Get recommendations for user\n",
    "user_id = 0  # First user\n",
    "recommendations = trainer.recommend_products(user_id, top_k=10)\n",
    "\n",
    "print(f\"Top recommendations for user {user_id}:\")\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec['product_name']}\")\n",
    "    print(f\"   Score: {rec['score']:.3f}, Link Prob: {rec['link_probability']:.3f}, Pred Qty: {rec['predicted_quantity']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03a97adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model - takes seconds\n",
    "model = TemporalGCNRecommender(\n",
    "    num_users=tkg_data['num_users'],\n",
    "    num_products=tkg_data['num_products'],\n",
    "    node_features_dim=tkg_data['x'].shape[1],\n",
    "    hidden_dim=64\n",
    ")\n",
    "model.load_state_dict(torch.load('best_temporal_recommender.pt'))\n",
    "\n",
    "# Get recommendations instantly (milliseconds)\n",
    "recommendations = trainer.recommend_products(user_id=1002, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80992ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'product_id': 3957,\n",
       "  'product_name': 'Product_3957',\n",
       "  'score': 1.9419025182724,\n",
       "  'link_probability': 0.8873864412307739,\n",
       "  'predicted_quantity': 2.1883392333984375},\n",
       " {'product_id': 3988,\n",
       "  'product_name': 'Product_3988',\n",
       "  'score': 1.9356331825256348,\n",
       "  'link_probability': 0.8947839140892029,\n",
       "  'predicted_quantity': 2.163240909576416},\n",
       " {'product_id': 3942,\n",
       "  'product_name': 'Product_3942',\n",
       "  'score': 1.9340131282806396,\n",
       "  'link_probability': 0.895521342754364,\n",
       "  'predicted_quantity': 2.1596505641937256},\n",
       " {'product_id': 3931,\n",
       "  'product_name': 'Product_3931',\n",
       "  'score': 1.931741714477539,\n",
       "  'link_probability': 0.8929761648178101,\n",
       "  'predicted_quantity': 2.163262367248535},\n",
       " {'product_id': 3932,\n",
       "  'product_name': 'Product_3932',\n",
       "  'score': 1.9310879707336426,\n",
       "  'link_probability': 0.8947371244430542,\n",
       "  'predicted_quantity': 2.1582741737365723},\n",
       " {'product_id': 3970,\n",
       "  'product_name': 'Product_3970',\n",
       "  'score': 1.9257551431655884,\n",
       "  'link_probability': 0.8819207549095154,\n",
       "  'predicted_quantity': 2.1835920810699463},\n",
       " {'product_id': 3958,\n",
       "  'product_name': 'Product_3958',\n",
       "  'score': 1.9254586696624756,\n",
       "  'link_probability': 0.8946363925933838,\n",
       "  'predicted_quantity': 2.1522247791290283},\n",
       " {'product_id': 3992,\n",
       "  'product_name': 'Product_3992',\n",
       "  'score': 1.9211515188217163,\n",
       "  'link_probability': 0.8872110247612,\n",
       "  'predicted_quantity': 2.1653828620910645},\n",
       " {'product_id': 3928,\n",
       "  'product_name': 'Product_3928',\n",
       "  'score': 1.9209195375442505,\n",
       "  'link_probability': 0.8968917727470398,\n",
       "  'predicted_quantity': 2.141751766204834},\n",
       " {'product_id': 3960,\n",
       "  'product_name': 'Product_3960',\n",
       "  'score': 1.9185370206832886,\n",
       "  'link_probability': 0.8896465301513672,\n",
       "  'predicted_quantity': 2.1565160751342773}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7471b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Customer: 12346.0\n",
      "- FELT EGG COSY CHICKEN: 7.33 units\n",
      "- JAM MAKING SET WITH JARS: 7.31 units\n",
      "- POPPY'S PLAYHOUSE BEDROOM : 7.33 units\n",
      "- POPPY'S PLAYHOUSE KITCHEN: 7.31 units\n",
      "- PACK OF 72 RETROSPOT CAKE CASES: 7.45 units\n",
      "- HOT WATER BOTTLE TEA AND SYMPATHY: 7.27 units\n",
      "- COOK WITH WINE METAL SIGN : 7.35 units\n",
      "- HAND WARMER UNION JACK: 7.21 units\n",
      "- JUMBO BAG PINK POLKADOT: 7.29 units\n",
      "- VICTORIAN SEWING BOX LARGE: 7.4 units\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "================================================================================\n",
      "PRODUCT RECOMMENDATIONS\n",
      "================================================================================\n",
      "Customer ID: 12346.0\n",
      "Internal ID: 1002\n",
      "Top 10 Recommendations:\n",
      "--------------------------------------------------------------------------------\n",
      " 1. Product: VICTORIAN SEWING BOX LARGE\n",
      "     Confidence Score: 1.942\n",
      "     Purchase Probability: 88.7%\n",
      "     Expected Quantity: 7.47\n",
      "     Product ID: 3957\n",
      "\n",
      " 2. Product: FELT EGG COSY CHICKEN\n",
      "     Confidence Score: 1.935\n",
      "     Purchase Probability: 89.5%\n",
      "     Expected Quantity: 7.33\n",
      "     Product ID: 3988\n",
      "\n",
      " 3. Product: JAM MAKING SET WITH JARS\n",
      "     Confidence Score: 1.934\n",
      "     Purchase Probability: 89.5%\n",
      "     Expected Quantity: 7.31\n",
      "     Product ID: 3942\n",
      "\n",
      " 4. Product: POPPY'S PLAYHOUSE KITCHEN\n",
      "     Confidence Score: 1.931\n",
      "     Purchase Probability: 89.5%\n",
      "     Expected Quantity: 7.31\n",
      "     Product ID: 3932\n",
      "\n",
      " 5. Product: POPPY'S PLAYHOUSE BEDROOM \n",
      "     Confidence Score: 1.931\n",
      "     Purchase Probability: 89.3%\n",
      "     Expected Quantity: 7.33\n",
      "     Product ID: 3931\n",
      "\n",
      " 6. Product: PACK OF 72 RETROSPOT CAKE CASES\n",
      "     Confidence Score: 1.927\n",
      "     Purchase Probability: 88.2%\n",
      "     Expected Quantity: 7.45\n",
      "     Product ID: 3970\n",
      "\n",
      " 7. Product: HOT WATER BOTTLE TEA AND SYMPATHY\n",
      "     Confidence Score: 1.925\n",
      "     Purchase Probability: 89.5%\n",
      "     Expected Quantity: 7.27\n",
      "     Product ID: 3958\n",
      "\n",
      " 8. Product: COOK WITH WINE METAL SIGN \n",
      "     Confidence Score: 1.921\n",
      "     Purchase Probability: 88.7%\n",
      "     Expected Quantity: 7.35\n",
      "     Product ID: 3992\n",
      "\n",
      " 9. Product: JUMBO BAG PINK POLKADOT\n",
      "     Confidence Score: 1.919\n",
      "     Purchase Probability: 89.0%\n",
      "     Expected Quantity: 7.3\n",
      "     Product ID: 3960\n",
      "\n",
      "10. Product: HAND WARMER UNION JACK\n",
      "     Confidence Score: 1.919\n",
      "     Purchase Probability: 89.7%\n",
      "     Expected Quantity: 7.2\n",
      "     Product ID: 3928\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# Find internal ID by customer ID\\ncustomer_id = '12345'  # Your actual customer ID\\ninternal_id = find_user_by_customer_id(customer_id, mappings_data)\\nif internal_id is not None:\\n    result = get_recommendations_with_actual_values(internal_id, trainer, mappings_data)\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_tkg_mappings(base_path='graph/my_retail_tkg'):\n",
    "    \"\"\"\n",
    "    Load TKG mappings and data from saved files\n",
    "    \n",
    "    Args:\n",
    "        base_path: Base path to the saved TKG files (without extension)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing all mappings and metadata\n",
    "    \"\"\"\n",
    "    # Load the main pickle file with all data\n",
    "    with open(f'{base_path}.pkl', 'rb') as f:\n",
    "        tkg_data = pickle.load(f)\n",
    "    \n",
    "    # Load the JSON mappings (for easy inspection)\n",
    "    with open(f'{base_path}_mappings.json', 'r') as f:\n",
    "        mappings_json = json.load(f)\n",
    "    \n",
    "    # Load PyTorch geometric data\n",
    "    pyg_data = torch.load(f'{base_path}_pyg.pt', weights_only=False)\n",
    "    \n",
    "    return {\n",
    "        'user_to_id': tkg_data['user_to_id'],\n",
    "        'product_to_id': tkg_data['product_to_id'],\n",
    "        'id_to_user': tkg_data['id_to_user'],\n",
    "        'id_to_product': tkg_data['id_to_product'],\n",
    "        'user_features': tkg_data['user_features'],\n",
    "        'product_features': tkg_data['product_features'],\n",
    "        'quadruples': tkg_data['quadruples'],\n",
    "        'quantity_scaler': tkg_data.get('quantity_scaler'),  # May not exist in older saves\n",
    "        'log_transform_used': tkg_data.get('log_transform_used', True),\n",
    "        'metadata': tkg_data['metadata'],\n",
    "        'pyg_data': pyg_data\n",
    "    }\n",
    "\n",
    "def reverse_quantity_scaling(normalized_quantities, mappings_data):\n",
    "    \"\"\"\n",
    "    Reverse the quantity scaling applied during TKG creation\n",
    "    \n",
    "    Args:\n",
    "        normalized_quantities: Array or single value of normalized quantities\n",
    "        mappings_data: Data loaded from load_tkg_mappings()\n",
    "    \n",
    "    Returns:\n",
    "        Actual quantities\n",
    "    \"\"\"\n",
    "    # Convert single value to array for processing\n",
    "    is_single = not isinstance(normalized_quantities, (list, np.ndarray))\n",
    "    if is_single:\n",
    "        normalized_quantities = [normalized_quantities]\n",
    "    \n",
    "    # If we have the original scaler, use it\n",
    "    if mappings_data.get('quantity_scaler') is not None:\n",
    "        quantity_scaler = mappings_data['quantity_scaler']\n",
    "        log_transform_used = mappings_data.get('log_transform_used', True)\n",
    "        \n",
    "        # Inverse min-max scaling\n",
    "        log_quantities = quantity_scaler.inverse_transform(\n",
    "            np.array(normalized_quantities).reshape(-1, 1)\n",
    "        ).flatten()\n",
    "        \n",
    "        # Inverse log transform if it was used\n",
    "        if log_transform_used:\n",
    "            actual_quantities = np.expm1(log_quantities)  # exp(x) - 1\n",
    "        else:\n",
    "            actual_quantities = log_quantities\n",
    "    else:\n",
    "        # Fallback: estimate reverse scaling based on typical range\n",
    "        # This is less accurate but works if scaler wasn't saved\n",
    "        print(\"Warning: Original scaler not found, using estimated reverse scaling\")\n",
    "        \n",
    "        # Assume normalized range was 1.0-10.0, map back to reasonable quantities\n",
    "        normalized_array = np.array(normalized_quantities)\n",
    "        # Simple linear mapping back to 1-50 range (adjust based on your data)\n",
    "        actual_quantities = 1 + (normalized_array - 1) * 49 / 9  # (10-1) = 9 is the norm range\n",
    "    \n",
    "    # Ensure minimum quantity of 1 and round to reasonable precision\n",
    "    actual_quantities = np.maximum(1, actual_quantities)\n",
    "    \n",
    "    return actual_quantities[0] if is_single else actual_quantities\n",
    "\n",
    "def map_recommendations_to_actual(recommendations, mappings_data):\n",
    "    \"\"\"\n",
    "    Map internal IDs and normalized values back to actual customer IDs, product descriptions, and quantities\n",
    "    \n",
    "    Args:\n",
    "        recommendations: List of recommendation dictionaries from the model\n",
    "        mappings_data: Data loaded from load_tkg_mappings()\n",
    "    \n",
    "    Returns:\n",
    "        List of recommendations with actual values\n",
    "    \"\"\"\n",
    "    mapped_recommendations = []\n",
    "    \n",
    "    for rec in recommendations:\n",
    "        # Map product ID to actual description\n",
    "        product_id = rec['product_id']\n",
    "        actual_description = mappings_data['id_to_product'].get(product_id, 'Unknown Product')\n",
    "        \n",
    "        # Map predicted quantity back to actual scale\n",
    "        predicted_qty_normalized = rec['predicted_quantity']\n",
    "        actual_qty = reverse_quantity_scaling(predicted_qty_normalized, mappings_data)\n",
    "        \n",
    "        mapped_rec = {\n",
    "            'product_id': product_id,\n",
    "            'product_description': actual_description,\n",
    "            'score': rec['score'],\n",
    "            'link_probability': rec['link_probability'],\n",
    "            'predicted_quantity_normalized': predicted_qty_normalized,\n",
    "            'predicted_quantity_actual': round(actual_qty, 2)\n",
    "        }\n",
    "        \n",
    "        mapped_recommendations.append(mapped_rec)\n",
    "    \n",
    "    return mapped_recommendations\n",
    "\n",
    "def map_user_id_to_actual(user_id, mappings_data):\n",
    "    \"\"\"\n",
    "    Map internal user ID back to actual customer ID\n",
    "    \n",
    "    Args:\n",
    "        user_id: Internal user ID used in the model\n",
    "        mappings_data: Data loaded from load_tkg_mappings()\n",
    "    \n",
    "    Returns:\n",
    "        Actual customer ID\n",
    "    \"\"\"\n",
    "    return mappings_data['id_to_user'].get(user_id, 'Unknown Customer')\n",
    "\n",
    "def get_recommendations_with_actual_values(user_id, trainer, mappings_data, top_k=10):\n",
    "    \"\"\"\n",
    "    Get recommendations and map all values back to actual identifiers\n",
    "    \n",
    "    Args:\n",
    "        user_id: Internal user ID\n",
    "        trainer: Your trained model/trainer instance\n",
    "        mappings_data: Data loaded from load_tkg_mappings()\n",
    "        top_k: Number of recommendations to get\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with actual customer ID and mapped recommendations\n",
    "    \"\"\"\n",
    "    # Get raw recommendations\n",
    "    raw_recommendations = trainer.recommend_products(user_id=user_id, top_k=top_k)\n",
    "    \n",
    "    # Map to actual values\n",
    "    mapped_recommendations = map_recommendations_to_actual(raw_recommendations, mappings_data)\n",
    "    \n",
    "    # Get actual customer ID\n",
    "    actual_customer_id = map_user_id_to_actual(user_id, mappings_data)\n",
    "    \n",
    "    return {\n",
    "        'customer_id_internal': user_id,\n",
    "        'customer_id_actual': actual_customer_id,\n",
    "        'recommendations': mapped_recommendations\n",
    "    }\n",
    "\n",
    "def create_recommendation_report(user_id, trainer, mappings_data, top_k=10):\n",
    "    \"\"\"\n",
    "    Create a formatted recommendation report with actual values\n",
    "    \"\"\"\n",
    "    result = get_recommendations_with_actual_values(user_id, trainer, mappings_data, top_k)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"PRODUCT RECOMMENDATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Customer ID: {result['customer_id_actual']}\")\n",
    "    print(f\"Internal ID: {result['customer_id_internal']}\")\n",
    "    print(f\"Top {top_k} Recommendations:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, rec in enumerate(result['recommendations'], 1):\n",
    "        print(f\"{i:2d}. Product: {rec['product_description']}\")\n",
    "        print(f\"     Confidence Score: {rec['score']:.3f}\")\n",
    "        print(f\"     Purchase Probability: {rec['link_probability']:.1%}\")\n",
    "        print(f\"     Expected Quantity: {rec['predicted_quantity_actual']}\")\n",
    "        print(f\"     Product ID: {rec['product_id']}\")\n",
    "        print()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_batch_recommendations(user_ids, trainer, mappings_data, top_k=10):\n",
    "    \"\"\"\n",
    "    Get recommendations for multiple users at once\n",
    "    \n",
    "    Args:\n",
    "        user_ids: List of internal user IDs\n",
    "        trainer: Your trained model/trainer instance  \n",
    "        mappings_data: Data loaded from load_tkg_mappings()\n",
    "        top_k: Number of recommendations per user\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping actual customer IDs to their recommendations\n",
    "    \"\"\"\n",
    "    batch_results = {}\n",
    "    \n",
    "    for user_id in user_ids:\n",
    "        result = get_recommendations_with_actual_values(user_id, trainer, mappings_data, top_k)\n",
    "        batch_results[result['customer_id_actual']] = result\n",
    "    \n",
    "    return batch_results\n",
    "\n",
    "def find_user_by_customer_id(customer_id, mappings_data):\n",
    "    \"\"\"\n",
    "    Find internal user ID by actual customer ID\n",
    "    \n",
    "    Args:\n",
    "        customer_id: Actual customer ID from your original data\n",
    "        mappings_data: Data loaded from load_tkg_mappings()\n",
    "        \n",
    "    Returns:\n",
    "        Internal user ID or None if not found\n",
    "    \"\"\"\n",
    "    return mappings_data['user_to_id'].get(customer_id)\n",
    "\n",
    "def find_product_by_description(description, mappings_data):\n",
    "    \"\"\"\n",
    "    Find internal product ID by product description\n",
    "    \n",
    "    Args:\n",
    "        description: Product description from your original data\n",
    "        mappings_data: Data loaded from load_tkg_mappings()\n",
    "        \n",
    "    Returns:\n",
    "        Internal product ID or None if not found\n",
    "    \"\"\"\n",
    "    return mappings_data['product_to_id'].get(description)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b0fd01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Customer: 12748.0\n",
      "- 5 STRAND GLASS NECKLACE CRYSTAL: 5.87 units\n",
      "- BLUE NEW BAROQUE CANDLESTICK CANDLE: 5.87 units\n",
      "- SET OF 3 COLOURED  FLYING DUCKS: 5.87 units\n",
      "- ROUND SNACK BOXES SET OF4 WOODLAND : 5.97 units\n",
      "- DISCO BALL CHRISTMAS DECORATION: 5.98 units\n",
      "- RETROSPOT LAMP: 5.97 units\n",
      "- SMALL HEART FLOWERS HOOK : 5.95 units\n",
      "- BLUE COAT RACK PARIS FASHION: 5.87 units\n",
      "- VICTORIAN SEWING BOX LARGE: 6.01 units\n",
      "- DOORMAT FAIRY CAKE: 5.88 units\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "Warning: Original scaler not found, using estimated reverse scaling\n",
      "================================================================================\n",
      "PRODUCT RECOMMENDATIONS\n",
      "================================================================================\n",
      "Customer ID: 12346.0\n",
      "Internal ID: 1002\n",
      "Top 10 Recommendations:\n",
      "--------------------------------------------------------------------------------\n",
      " 1. Product: POPPY'S PLAYHOUSE KITCHEN\n",
      "     Confidence Score: 1.938\n",
      "     Purchase Probability: 89.5%\n",
      "     Expected Quantity: 7.35\n",
      "     Product ID: 3932\n",
      "\n",
      " 2. Product: JAM MAKING SET WITH JARS\n",
      "     Confidence Score: 1.927\n",
      "     Purchase Probability: 89.2%\n",
      "     Expected Quantity: 7.32\n",
      "     Product ID: 3942\n",
      "\n",
      " 3. Product: POPPY'S PLAYHOUSE BEDROOM \n",
      "     Confidence Score: 1.927\n",
      "     Purchase Probability: 89.3%\n",
      "     Expected Quantity: 7.3\n",
      "     Product ID: 3931\n",
      "\n",
      " 4. Product: PACK OF 72 RETROSPOT CAKE CASES\n",
      "     Confidence Score: 1.926\n",
      "     Purchase Probability: 88.2%\n",
      "     Expected Quantity: 7.45\n",
      "     Product ID: 3970\n",
      "\n",
      " 5. Product: HOT WATER BOTTLE TEA AND SYMPATHY\n",
      "     Confidence Score: 1.924\n",
      "     Purchase Probability: 89.4%\n",
      "     Expected Quantity: 7.27\n",
      "     Product ID: 3958\n",
      "\n",
      " 6. Product: COOK WITH WINE METAL SIGN \n",
      "     Confidence Score: 1.921\n",
      "     Purchase Probability: 88.7%\n",
      "     Expected Quantity: 7.34\n",
      "     Product ID: 3992\n",
      "\n",
      " 7. Product: HAND WARMER UNION JACK\n",
      "     Confidence Score: 1.920\n",
      "     Purchase Probability: 89.6%\n",
      "     Expected Quantity: 7.23\n",
      "     Product ID: 3928\n",
      "\n",
      " 8. Product: FELT EGG COSY CHICKEN\n",
      "     Confidence Score: 1.919\n",
      "     Purchase Probability: 89.5%\n",
      "     Expected Quantity: 7.23\n",
      "     Product ID: 3988\n",
      "\n",
      " 9. Product: JUMBO BAG PINK POLKADOT\n",
      "     Confidence Score: 1.919\n",
      "     Purchase Probability: 89.0%\n",
      "     Expected Quantity: 7.3\n",
      "     Product ID: 3960\n",
      "\n",
      "10. Product: VICTORIAN SEWING BOX LARGE\n",
      "     Confidence Score: 1.917\n",
      "     Purchase Probability: 88.1%\n",
      "     Expected Quantity: 7.4\n",
      "     Product ID: 3957\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# Find internal ID by customer ID\\ncustomer_id = '12345'  # Your actual customer ID\\ninternal_id = find_user_by_customer_id(customer_id, mappings_data)\\nif internal_id is not None:\\n    result = get_recommendations_with_actual_values(internal_id, trainer, mappings_data)\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usage example:\n",
    "\n",
    "# Load all mappings and data\n",
    "mappings_data = load_tkg_mappings('graph/my_retail_tkg')\n",
    "\n",
    "# Get recommendations with actual values\n",
    "result = get_recommendations_with_actual_values(\n",
    "    user_id=43, \n",
    "    trainer=trainer, \n",
    "    mappings_data=mappings_data, \n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "print(f\"Customer: {result['customer_id_actual']}\")\n",
    "for rec in result['recommendations']:\n",
    "    print(f\"- {rec['product_description']}: {rec['predicted_quantity_actual']} units\")\n",
    "\n",
    "# Or create a formatted report\n",
    "create_recommendation_report(user_id=1002, trainer=trainer, mappings_data=mappings_data)\n",
    "\"\"\"\n",
    "# Find internal ID by customer ID\n",
    "customer_id = '12345'  # Your actual customer ID\n",
    "internal_id = find_user_by_customer_id(customer_id, mappings_data)\n",
    "if internal_id is not None:\n",
    "    result = get_recommendations_with_actual_values(internal_id, trainer, mappings_data)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642e8419",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
