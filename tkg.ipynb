{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52b96efc",
   "metadata": {},
   "source": [
    "# Temporal Graph \n",
    "\n",
    "## Basic Structure\n",
    "\n",
    "Bipartite Graph\n",
    "Two Types Of Nodes:\n",
    "User Nodes (This will be CustomerId)- as of now we will not be adding features to this \n",
    "\n",
    "Product Nodes (This will be Description)- this will have a feature will be the unit price of the product\n",
    "\n",
    "Timestep- a day \n",
    "\n",
    "Edge weight- Quantity \n",
    "Edge Attribute- Timestamp (Invoice Date)\n",
    "\n",
    "So we will essentially have a quadruple like standard tkgs do\n",
    "(U, E, P, T): User, Edge (Qty), Product, Timestamp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17a60295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d3c8af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_df=pd.read_csv('data/uk_retail_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca247e37",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "785ba34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 133600 rows with missing values\n"
     ]
    }
   ],
   "source": [
    "# Remove missing values\n",
    "initial_rows = len(uk_df)\n",
    "uk_df = uk_df.dropna(subset=['CustomerID', 'Description', 'Quantity', 'InvoiceDate', 'UnitPrice'])\n",
    "print(f\"Removed {initial_rows - len(uk_df)} rows with missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27041a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove negative quantities\n",
    "uk_df = uk_df[uk_df['Quantity'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8041f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert InvoiceDate to datetime\n",
    "uk_df['InvoiceDate'] = pd.to_datetime(uk_df['InvoiceDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc92883a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536365</td>\n",
       "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
       "      <td>6</td>\n",
       "      <td>2010-12-01</td>\n",
       "      <td>2.55</td>\n",
       "      <td>17850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536365</td>\n",
       "      <td>WHITE METAL LANTERN</td>\n",
       "      <td>6</td>\n",
       "      <td>2010-12-01</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536365</td>\n",
       "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
       "      <td>8</td>\n",
       "      <td>2010-12-01</td>\n",
       "      <td>2.75</td>\n",
       "      <td>17850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536365</td>\n",
       "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
       "      <td>6</td>\n",
       "      <td>2010-12-01</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>536365</td>\n",
       "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
       "      <td>6</td>\n",
       "      <td>2010-12-01</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495473</th>\n",
       "      <td>581585</td>\n",
       "      <td>FAIRY TALE COTTAGE NIGHT LIGHT</td>\n",
       "      <td>12</td>\n",
       "      <td>2011-12-09</td>\n",
       "      <td>1.95</td>\n",
       "      <td>15804.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495474</th>\n",
       "      <td>581586</td>\n",
       "      <td>LARGE CAKE STAND  HANGING STRAWBERY</td>\n",
       "      <td>8</td>\n",
       "      <td>2011-12-09</td>\n",
       "      <td>2.95</td>\n",
       "      <td>13113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495475</th>\n",
       "      <td>581586</td>\n",
       "      <td>SET OF 3 HANGING OWLS OLLIE BEAK</td>\n",
       "      <td>24</td>\n",
       "      <td>2011-12-09</td>\n",
       "      <td>1.25</td>\n",
       "      <td>13113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495476</th>\n",
       "      <td>581586</td>\n",
       "      <td>RED RETROSPOT ROUND CAKE TINS</td>\n",
       "      <td>24</td>\n",
       "      <td>2011-12-09</td>\n",
       "      <td>8.95</td>\n",
       "      <td>13113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495477</th>\n",
       "      <td>581586</td>\n",
       "      <td>DOORMAT RED RETROSPOT</td>\n",
       "      <td>10</td>\n",
       "      <td>2011-12-09</td>\n",
       "      <td>7.08</td>\n",
       "      <td>13113.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>354345 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       InvoiceNo                          Description  Quantity InvoiceDate  \\\n",
       "0         536365   WHITE HANGING HEART T-LIGHT HOLDER         6  2010-12-01   \n",
       "1         536365                  WHITE METAL LANTERN         6  2010-12-01   \n",
       "2         536365       CREAM CUPID HEARTS COAT HANGER         8  2010-12-01   \n",
       "3         536365  KNITTED UNION FLAG HOT WATER BOTTLE         6  2010-12-01   \n",
       "4         536365       RED WOOLLY HOTTIE WHITE HEART.         6  2010-12-01   \n",
       "...          ...                                  ...       ...         ...   \n",
       "495473    581585       FAIRY TALE COTTAGE NIGHT LIGHT        12  2011-12-09   \n",
       "495474    581586  LARGE CAKE STAND  HANGING STRAWBERY         8  2011-12-09   \n",
       "495475    581586     SET OF 3 HANGING OWLS OLLIE BEAK        24  2011-12-09   \n",
       "495476    581586        RED RETROSPOT ROUND CAKE TINS        24  2011-12-09   \n",
       "495477    581586                DOORMAT RED RETROSPOT        10  2011-12-09   \n",
       "\n",
       "        UnitPrice  CustomerID  \n",
       "0            2.55     17850.0  \n",
       "1            3.39     17850.0  \n",
       "2            2.75     17850.0  \n",
       "3            3.39     17850.0  \n",
       "4            3.39     17850.0  \n",
       "...           ...         ...  \n",
       "495473       1.95     15804.0  \n",
       "495474       2.95     13113.0  \n",
       "495475       1.25     13113.0  \n",
       "495476       8.95     13113.0  \n",
       "495477       7.08     13113.0  \n",
       "\n",
       "[354345 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96d06731",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalKnowledgeGraphBuilder:\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Initialize TKG builder with retail dataset\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with columns [InvoiceNo, Description, Quantity, InvoiceDate, UnitPrice, CustomerID]\n",
    "        \"\"\"\n",
    "        self.df = df.copy()\n",
    "        self.user_to_id = {}\n",
    "        self.product_to_id = {}\n",
    "        self.id_to_user = {}\n",
    "        self.id_to_product = {}\n",
    "        \n",
    "        # TKG components\n",
    "        self.quadruples = []  # (user_id, edge_weight, product_id, timestamp)\n",
    "        self.user_features = None\n",
    "        self.product_features = None\n",
    "        \n",
    "    def create_node_mappings(self):\n",
    "        \"\"\"Create bidirectional mappings between entities and IDs\"\"\"\n",
    "        print(\"Creating node mappings...\")\n",
    "        \n",
    "        # Get unique users and products\n",
    "        unique_users = self.df['CustomerID'].unique()\n",
    "        unique_products = self.df['Description'].unique()\n",
    "        \n",
    "        # Create user mappings\n",
    "        self.user_to_id = {user: idx for idx, user in enumerate(unique_users)}\n",
    "        self.id_to_user = {idx: user for user, idx in self.user_to_id.items()}\n",
    "        \n",
    "        # Create product mappings (offset by number of users for bipartite structure)\n",
    "        offset = len(unique_users)\n",
    "        self.product_to_id = {product: idx + offset for idx, product in enumerate(unique_products)}\n",
    "        self.id_to_product = {idx: product for product, idx in self.product_to_id.items()}\n",
    "        \n",
    "        print(f\"Created mappings for {len(unique_users)} users and {len(unique_products)} products\")\n",
    "        \n",
    "    def create_node_features(self):\n",
    "        \"\"\"Create node features with better scaling\"\"\"\n",
    "        print(\"Creating node features...\")\n",
    "        \n",
    "        num_users = len(self.user_to_id)\n",
    "        num_products = len(self.product_to_id)\n",
    "        \n",
    "        # User features: For now, just use random features or purchase statistics\n",
    "        user_stats = self.df.groupby('CustomerID').agg({\n",
    "            'Quantity': 'sum',\n",
    "            'UnitPrice': 'mean',\n",
    "            'InvoiceNo': 'nunique'  # Number of unique invoices\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Initialize user features matrix\n",
    "        self.user_features = np.zeros((num_users, 3))  # [total_qty, avg_price, num_invoices]\n",
    "        \n",
    "        for _, row in user_stats.iterrows():\n",
    "            user_idx = self.user_to_id[row['CustomerID']]\n",
    "            self.user_features[user_idx] = [\n",
    "                row['Quantity'],\n",
    "                row['UnitPrice'],\n",
    "                row['InvoiceNo']\n",
    "            ]\n",
    "        \n",
    "        # Use min-max scaling instead of z-score to avoid very small values\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        user_scaler = MinMaxScaler(feature_range=(0.1, 1.0))  # Avoid zeros\n",
    "        self.user_features = user_scaler.fit_transform(self.user_features)\n",
    "        \n",
    "        # Product features: Unit price and popularity\n",
    "        product_stats = self.df.groupby('Description').agg({\n",
    "            'UnitPrice': 'mean',\n",
    "            'Quantity': 'sum',\n",
    "            'CustomerID': 'nunique'  # Number of unique customers\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Initialize product features matrix\n",
    "        self.product_features = np.zeros((num_products, 3))  # [avg_price, total_sold, num_customers]\n",
    "        \n",
    "        for _, row in product_stats.iterrows():\n",
    "            product_idx = self.product_to_id[row['Description']] - len(self.user_to_id)  # Remove offset for indexing\n",
    "            self.product_features[product_idx] = [\n",
    "                row['UnitPrice'],\n",
    "                row['Quantity'],\n",
    "                row['CustomerID']\n",
    "            ]\n",
    "        \n",
    "        # Use min-max scaling for products too\n",
    "        product_scaler = MinMaxScaler(feature_range=(0.1, 1.0))\n",
    "        self.product_features = product_scaler.fit_transform(self.product_features)\n",
    "        \n",
    "        print(f\"User features shape: {self.user_features.shape}\")\n",
    "        print(f\"Product features shape: {self.product_features.shape}\")\n",
    "\n",
    "    def create_quadruples(self):\n",
    "        \"\"\"Create TKG quadruples with scaled quantities\"\"\"\n",
    "        print(\"Creating temporal quadruples...\")\n",
    "        \n",
    "        self.quadruples = []\n",
    "        quantities = self.df['Quantity'].values\n",
    "        \n",
    "        # Scale quantities to reasonable range (log transform + min-max scaling)\n",
    "        log_quantities = np.log1p(quantities)  # log(1 + x) to handle zeros\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        qty_scaler = MinMaxScaler(feature_range=(1.0, 10.0))  # Scale to meaningful range\n",
    "        scaled_quantities = qty_scaler.fit_transform(log_quantities.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Store scaler for inverse transform if needed\n",
    "        self.quantity_scaler = qty_scaler\n",
    "        self.log_transform_used = True\n",
    "        \n",
    "        for i, (_, row) in enumerate(self.df.iterrows()):\n",
    "            user_id = self.user_to_id[row['CustomerID']]\n",
    "            product_id = self.product_to_id[row['Description']]\n",
    "            quantity = scaled_quantities[i]  # Use scaled quantity\n",
    "            timestamp = row['InvoiceDate']\n",
    "            \n",
    "            # Create quadruple: (subject, relation_weight, object, time)\n",
    "            quadruple = (user_id, quantity, product_id, timestamp)\n",
    "            self.quadruples.append(quadruple)\n",
    "        \n",
    "        # Sort by timestamp for temporal consistency\n",
    "        self.quadruples.sort(key=lambda x: x[3])\n",
    "        \n",
    "        print(f\"Created {len(self.quadruples)} temporal quadruples\")\n",
    "        print(f\"Quantity range: {min(q[1] for q in self.quadruples):.3f} to {max(q[1] for q in self.quadruples):.3f}\")\n",
    "        \n",
    "    def get_pytorch_geometric_format(self):\n",
    "        \"\"\"Convert TKG to PyTorch Geometric format\"\"\"\n",
    "        \n",
    "        # Create edge index (bipartite graph structure)\n",
    "        edge_index = []\n",
    "        edge_attr = []  # [quantity, timestamp_encoded]\n",
    "        \n",
    "        # Convert timestamps to numerical format (days since first transaction)\n",
    "        min_date = min(q[3] for q in self.quadruples)\n",
    "        \n",
    "        for user_id, quantity, product_id, timestamp in self.quadruples:\n",
    "            # Add edge from user to product\n",
    "            edge_index.append([user_id, product_id])\n",
    "            \n",
    "            # Encode timestamp as days since start\n",
    "            days_since_start = (timestamp - min_date).days\n",
    "            edge_attr.append([quantity, days_since_start])\n",
    "        \n",
    "        # Convert to tensors\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "        \n",
    "        # Combine user and product features\n",
    "        all_node_features = np.vstack([self.user_features, self.product_features])\n",
    "        node_features = torch.tensor(all_node_features, dtype=torch.float)\n",
    "        \n",
    "        return {\n",
    "            'edge_index': edge_index,\n",
    "            'edge_attr': edge_attr,\n",
    "            'x': node_features,\n",
    "            'num_users': len(self.user_to_id),\n",
    "            'num_products': len(self.product_to_id),\n",
    "            'quadruples': self.quadruples\n",
    "        }\n",
    "    \n",
    "    def analyze_temporal_patterns(self):\n",
    "        \"\"\"Analyze temporal patterns in the data\"\"\"\n",
    "        \n",
    "        # Convert quadruples to DataFrame for analysis\n",
    "        quad_df = pd.DataFrame(self.quadruples, columns=['user_id', 'quantity', 'product_id', 'timestamp'])\n",
    "        \n",
    "        # Temporal distribution\n",
    "        quad_df['hour'] = quad_df['timestamp'].dt.hour\n",
    "        quad_df['day_of_week'] = quad_df['timestamp'].dt.dayofweek\n",
    "        quad_df['month'] = quad_df['timestamp'].dt.month\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Transactions by hour\n",
    "        hour_counts = quad_df['hour'].value_counts().sort_index()\n",
    "        axes[0, 0].bar(hour_counts.index, hour_counts.values)\n",
    "        axes[0, 0].set_title('Transactions by Hour of Day')\n",
    "        axes[0, 0].set_xlabel('Hour')\n",
    "        axes[0, 0].set_ylabel('Number of Transactions')\n",
    "        \n",
    "        # Transactions by day of week\n",
    "        day_counts = quad_df['day_of_week'].value_counts().sort_index()\n",
    "        day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "        axes[0, 1].bar([day_names[i] for i in day_counts.index], day_counts.values)\n",
    "        axes[0, 1].set_title('Transactions by Day of Week')\n",
    "        axes[0, 1].set_ylabel('Number of Transactions')\n",
    "        \n",
    "        # Transactions over time\n",
    "        daily_counts = quad_df.groupby(quad_df['timestamp'].dt.date).size()\n",
    "        axes[1, 0].plot(daily_counts.index, daily_counts.values)\n",
    "        axes[1, 0].set_title('Daily Transaction Volume')\n",
    "        axes[1, 0].set_xlabel('Date')\n",
    "        axes[1, 0].set_ylabel('Number of Transactions')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Quantity distribution\n",
    "        axes[1, 1].hist(quad_df['quantity'], bins=50, alpha=0.7)\n",
    "        axes[1, 1].set_title('Distribution of Purchase Quantities')\n",
    "        axes[1, 1].set_xlabel('Quantity')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return quad_df\n",
    "    \n",
    "    def build_tkg(self):\n",
    "        \"\"\"Main method to build the complete TKG\"\"\"\n",
    "        print(\"Building Temporal Knowledge Graph...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        self.create_node_mappings()\n",
    "        self.create_node_features()\n",
    "        self.create_quadruples()\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "        print(\"TKG Construction Complete!\")\n",
    "        print(f\"Graph Statistics:\")\n",
    "        print(f\"  - Users: {len(self.user_to_id)}\")\n",
    "        print(f\"  - Products: {len(self.product_to_id)}\")\n",
    "        print(f\"  - Temporal Edges: {len(self.quadruples)}\")\n",
    "        print(f\"  - Time Range: {min(q[3] for q in self.quadruples)} to {max(q[3] for q in self.quadruples)}\")\n",
    "        return self.get_pytorch_geometric_format()\n",
    "        \n",
    "    def save_tkg(self, filepath='tkg_dataset'):\n",
    "        \"\"\"\n",
    "        Save the complete TKG to disk for future use\n",
    "        \n",
    "        Args:\n",
    "            filepath: Base path for saving files (without extension)\n",
    "        \"\"\"\n",
    "        print(f\"Saving TKG to {filepath}...\")\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(filepath) if os.path.dirname(filepath) else '.', exist_ok=True)\n",
    "        \n",
    "        # Save all components\n",
    "        tkg_data = {\n",
    "            'quadruples': self.quadruples,\n",
    "            'user_to_id': self.user_to_id,\n",
    "            'product_to_id': self.product_to_id,\n",
    "            'id_to_user': self.id_to_user,\n",
    "            'id_to_product': self.id_to_product,\n",
    "            'user_features': self.user_features,\n",
    "            'product_features': self.product_features,\n",
    "            'metadata': {\n",
    "                'num_users': len(self.user_to_id),\n",
    "                'num_products': len(self.product_to_id),\n",
    "                'num_edges': len(self.quadruples),\n",
    "                'created_at': datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save as pickle (most efficient)\n",
    "        with open(f'{filepath}.pkl', 'wb') as f:\n",
    "            pickle.dump(tkg_data, f)\n",
    "        \n",
    "        # Also save mappings as JSON for human readability\n",
    "        mappings = {\n",
    "            'user_to_id': {str(k): v for k, v in self.user_to_id.items()},\n",
    "            'product_to_id': {str(k): v for k, v in self.product_to_id.items()},\n",
    "            'metadata': tkg_data['metadata']\n",
    "        }\n",
    "        \n",
    "        with open(f'{filepath}_mappings.json', 'w') as f:\n",
    "            json.dump(mappings, f, indent=2)\n",
    "        \n",
    "        # Save PyTorch format separately\n",
    "        pyg_data = self.get_pytorch_geometric_format()\n",
    "        torch.save(pyg_data, f'{filepath}_pyg.pt')\n",
    "        \n",
    "        print(f\"TKG saved successfully!\")\n",
    "        print(f\"  - Main data: {filepath}.pkl\")\n",
    "        print(f\"  - Mappings: {filepath}_mappings.json\") \n",
    "        print(f\"  - PyG format: {filepath}_pyg.pt\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load_tkg(cls, filepath='tkg_dataset'):\n",
    "        \"\"\"\n",
    "        Load a previously saved TKG\n",
    "        \n",
    "        Args:\n",
    "            filepath: Base path of saved files (without extension)\n",
    "            \n",
    "        Returns:\n",
    "            TemporalKnowledgeGraphBuilder instance with loaded data\n",
    "        \"\"\"\n",
    "        print(f\"Loading TKG from {filepath}...\")\n",
    "        \n",
    "        # Create empty instance\n",
    "        instance = cls(pd.DataFrame())  # Empty df, will be overwritten\n",
    "        \n",
    "        # Load main data\n",
    "        with open(f'{filepath}.pkl', 'rb') as f:\n",
    "            tkg_data = pickle.load(f)\n",
    "        \n",
    "        # Restore all attributes\n",
    "        instance.quadruples = tkg_data['quadruples']\n",
    "        instance.user_to_id = tkg_data['user_to_id']\n",
    "        instance.product_to_id = tkg_data['product_to_id']\n",
    "        instance.id_to_user = tkg_data['id_to_user']\n",
    "        instance.id_to_product = tkg_data['id_to_product']\n",
    "        instance.user_features = tkg_data['user_features']\n",
    "        instance.product_features = tkg_data['product_features']\n",
    "        \n",
    "        metadata = tkg_data['metadata']\n",
    "        print(f\"Loaded TKG created on {metadata['created_at']}\")\n",
    "        print(f\"  - Users: {metadata['num_users']}\")\n",
    "        print(f\"  - Products: {metadata['num_products']}\")\n",
    "        print(f\"  - Temporal Edges: {metadata['num_edges']}\")\n",
    "        \n",
    "        return instance\n",
    "    \n",
    "    def update_tkg(self, new_df, save_path=None):\n",
    "        \"\"\"\n",
    "        Update existing TKG with new data (incremental updates)\n",
    "        \n",
    "        Args:\n",
    "            new_df: New transaction data with same schema\n",
    "            save_path: If provided, save updated TKG to this path\n",
    "        \"\"\"\n",
    "        print(\"Updating TKG with new data...\")\n",
    "        \n",
    "        # Preprocess new data\n",
    "        new_df = new_df.dropna(subset=['CustomerID', 'Description', 'Quantity', 'InvoiceDate', 'UnitPrice'])\n",
    "        new_df = new_df[new_df['Quantity'] > 0]\n",
    "        new_df['InvoiceDate'] = pd.to_datetime(new_df['InvoiceDate'])\n",
    "        \n",
    "        new_quadruples = []\n",
    "        new_users = set()\n",
    "        new_products = set()\n",
    "        \n",
    "        for _, row in new_df.iterrows():\n",
    "            customer_id = row['CustomerID']\n",
    "            description = row['Description']\n",
    "            \n",
    "            # Handle new users\n",
    "            if customer_id not in self.user_to_id:\n",
    "                new_user_id = len(self.user_to_id)\n",
    "                self.user_to_id[customer_id] = new_user_id\n",
    "                self.id_to_user[new_user_id] = customer_id\n",
    "                new_users.add(customer_id)\n",
    "            \n",
    "            # Handle new products\n",
    "            if description not in self.product_to_id:\n",
    "                new_product_id = len(self.user_to_id) + len(self.product_to_id)\n",
    "                self.product_to_id[description] = new_product_id\n",
    "                self.id_to_product[new_product_id] = description\n",
    "                new_products.add(description)\n",
    "            \n",
    "            # Create quadruple\n",
    "            user_id = self.user_to_id[customer_id]\n",
    "            product_id = self.product_to_id[description]\n",
    "            quadruple = (user_id, row['Quantity'], product_id, row['InvoiceDate'])\n",
    "            new_quadruples.append(quadruple)\n",
    "        \n",
    "        # Add new quadruples and re-sort\n",
    "        self.quadruples.extend(new_quadruples)\n",
    "        self.quadruples.sort(key=lambda x: x[3])\n",
    "        \n",
    "        # Update features (this is simplified - you might want more sophisticated updates)\n",
    "        if new_users or new_products:\n",
    "            print(\"Recalculating features for updated graph...\")\n",
    "            # For simplicity, recalculate all features\n",
    "            # In production, you'd want incremental feature updates\n",
    "            combined_df = pd.concat([self.df, new_df]) if hasattr(self, 'df') else new_df\n",
    "            self.df = combined_df\n",
    "            self.create_node_features()\n",
    "        \n",
    "        print(f\"Added {len(new_quadruples)} new edges\")\n",
    "        print(f\"New users: {len(new_users)}, New products: {len(new_products)}\")\n",
    "        \n",
    "        if save_path:\n",
    "            self.save_tkg(save_path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_pytorch_format(filepath='tkg_dataset_pyg.pt'):\n",
    "        \"\"\"Quick load just the PyTorch Geometric format\"\"\"\n",
    "        return torch.load(filepath, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9ba7577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Temporal Knowledge Graph...\n",
      "==================================================\n",
      "Creating node mappings...\n",
      "Created mappings for 3921 users and 3844 products\n",
      "Creating node features...\n",
      "User features shape: (3921, 3)\n",
      "Product features shape: (3844, 3)\n",
      "Creating temporal quadruples...\n",
      "Created 354345 temporal quadruples\n",
      "Quantity range: 1.000 to 10.000\n",
      "==================================================\n",
      "TKG Construction Complete!\n",
      "Graph Statistics:\n",
      "  - Users: 3921\n",
      "  - Products: 3844\n",
      "  - Temporal Edges: 354345\n",
      "  - Time Range: 2010-12-01 00:00:00 to 2011-12-09 00:00:00\n",
      "Saving TKG to graph/my_retail_tkg...\n",
      "TKG saved successfully!\n",
      "  - Main data: graph/my_retail_tkg.pkl\n",
      "  - Mappings: graph/my_retail_tkg_mappings.json\n",
      "  - PyG format: graph/my_retail_tkg_pyg.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Later sessions - Just load the saved TKG\\ntkg_builder = TemporalKnowledgeGraphBuilder.load_tkg(\\'my_retail_tkg\\')\\ntkg_data = tkg_builder.get_pytorch_geometric_format()\\n\\n# Or quickly load just PyG format\\npyg_data = TemporalKnowledgeGraphBuilder.load_pytorch_format(\\'my_retail_tkg_pyg.pt\\')\\n\\n# Update with new data\\nnew_transactions = pd.read_csv(\\'new_data.csv\\')\\ntkg_builder.update_tkg(new_transactions, save_path=\\'my_retail_tkg_updated\\')\\n\\nprint(\"TKG ready for temporal recommendation modeling!\")\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage with persistence:\n",
    "\n",
    "tkg_builder = TemporalKnowledgeGraphBuilder(uk_df)\n",
    "tkg_data = tkg_builder.build_tkg()\n",
    "\n",
    "# Save for future use\n",
    "tkg_builder.save_tkg('graph/my_retail_tkg')\n",
    "\"\"\"\n",
    "# Later sessions - Just load the saved TKG\n",
    "tkg_builder = TemporalKnowledgeGraphBuilder.load_tkg('my_retail_tkg')\n",
    "tkg_data = tkg_builder.get_pytorch_geometric_format()\n",
    "\n",
    "# Or quickly load just PyG format\n",
    "pyg_data = TemporalKnowledgeGraphBuilder.load_pytorch_format('my_retail_tkg_pyg.pt')\n",
    "\n",
    "# Update with new data\n",
    "new_transactions = pd.read_csv('new_data.csv')\n",
    "tkg_builder.update_tkg(new_transactions, save_path='my_retail_tkg_updated')\n",
    "\n",
    "print(\"TKG ready for temporal recommendation modeling!\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdbd10a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TKG from graph/my_retail_tkg...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'graph/my_retail_tkg.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tkg_builder = \u001b[43mTemporalKnowledgeGraphBuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_tkg\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgraph/my_retail_tkg\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m tkg_data = tkg_builder.get_pytorch_geometric_format()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 295\u001b[39m, in \u001b[36mTemporalKnowledgeGraphBuilder.load_tkg\u001b[39m\u001b[34m(cls, filepath)\u001b[39m\n\u001b[32m    292\u001b[39m instance = \u001b[38;5;28mcls\u001b[39m(pd.DataFrame())  \u001b[38;5;66;03m# Empty df, will be overwritten\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[38;5;66;03m# Load main data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfilepath\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.pkl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    296\u001b[39m     tkg_data = pickle.load(f)\n\u001b[32m    298\u001b[39m \u001b[38;5;66;03m# Restore all attributes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KUNJAL PATWARI\\Yucca_Personal\\Graph_Recommender\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'graph/my_retail_tkg.pkl'"
     ]
    }
   ],
   "source": [
    "tkg_builder = TemporalKnowledgeGraphBuilder.load_tkg('graph/my_retail_tkg')\n",
    "tkg_data = tkg_builder.get_pytorch_geometric_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbb04d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyg_data = torch.load('graph/my_retail_tkg_pyg.pt', weights_only=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
